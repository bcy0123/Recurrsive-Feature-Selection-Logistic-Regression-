# Recurrsive-Feature-Selection-Logistic-Regression-
Data extraction and exploration This is a brief analysis of the dataset on each pitch. The atbat data is also joined with the pitches data to get information from 7 additional columns. This is a left join between pitches and atbat data (i-e pitches LEFT JOIN atbats) so it contains all the rows in the pitches data. The first plot is a correlation plot where darker red tones show a positive correlation  among the variables while a darker blue color show a negative correlation. Greyish tones show no or poor correlation.  A few subsequent plots show scores of home and away games and how they vary with attendance and delay in the start of the  game. These plots show relationships and distribution of the data as well as position of outliers. For example, attendance  between 20,000 and 30,000 seems to be correlated with both away and home games that have high scores. This exploration can be further built upon as well.   Data manipulation The feature of interest is the "Event" variable. This is the outcome of each pitch. There are 30 possible events. This makes the analysis complicated. This feature is converted into a binary feature with the value of 0 if the event is a 'Single', 'Walk', 'Double', 'Home Run' ,'Hit By Pitch', 'Field Error' ,'Intent Walk' or a 'Triple' and a value of 1 otherwise. Moreover, all meaningless variables that do not contribute to the correlations or variation in the data are dropped. This includes certain keys/IDs and certain categorical variables. The remaining numerical variables are then brought to a single scale. The scaling has a major impact on the modeling and analysis that is to follow.  Initial model The initial model consists of 45 features and almost 3 million rows of data. The data is split into 2 partitions; a training set and a testing set in a 70:30 respective ration. A logistic regression algorithm is trained on the training dataset on this data. It can be seen that the logistic regression algorithm performs well on the training data. The accuracy is  99.4%. However there are a large number of features that are difficult to analyse and can cause overfitting to the noise in the training data. A recursive algorithm that drops 1 weak variable in each iteration is also used. This algorithm reduces the number of  features to 15, without any decrease in training accuracy. Perhaps, all the 45 features are not required for the training of the algorithm and the features can be decreased even below 15 for further optimization. The algorithm is not tested  on the testing set yet. The goal is to improve the algorithm using the training set and then test it. This would ensure  the integrity of the algorithm on the testing set.  
